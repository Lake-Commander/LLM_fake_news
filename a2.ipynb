{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9d3e8c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe9da6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Utilities\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')                        # Tokenizer\n",
    "nltk.download('stopwords')                    # English stopwords\n",
    "nltk.download('wordnet')                      # Lemmatization\n",
    "nltk.download('omw-1.4')                      # WordNet synonyms\n",
    "nltk.download('averaged_perceptron_tagger')   # POS tagging\n",
    "nltk.download('punkt_tab')                    # Tokenizer for tab-separated text (Not standard, but useful for some datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37736206",
   "metadata": {},
   "source": [
    "## Prem Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
    "\n",
    "# Initial inspection\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts()) \n",
    "\n",
    "# Check for missing values in all columns\n",
    "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
    "\n",
    "# Define cleaning function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|[^a-zA-Z]\", \" \", str(text).lower())\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning (This is temporary)\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Identify rows with empty or whitespace-only clean_text\n",
    "empty_clean_rows = df[df['clean_text'].str.strip() == '']\n",
    "print(f\"\\nRows with empty clean_text after preprocessing: {len(empty_clean_rows)}\")\n",
    "print(empty_clean_rows.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d975633",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
    "\n",
    "# Drop unnamed column\n",
    "df.drop(columns=[col for col in df.columns if \"unnamed\" in col.lower()], inplace=True)\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text cleaning and normalization\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    # Fix encoding issues\n",
    "    text = fix_text(text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove any garbled leftovers\n",
    "    text = re.sub(r'[√¢‚Ç¨‚Ñ¢‚Äú‚Äù‚Äò‚Äô‚Äì‚Äî‚Ä¢‚Ä∫‚Äπ¬´¬ª]', '', text)\n",
    "    # Remove non-alphabetic characters (numbers, symbols)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Apply cleaning to 'text' column\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Word Count feature\n",
    "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Sentiment polarity (TextBlob)\n",
    "df['sentiment'] = df['clean_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Term Frequency Matrix (CountVectorizer)\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "term_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Create word cloud for FAKE news articles\n",
    "fake_text = ' '.join(df[df['label'] == 'FAKE']['clean_text'])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(fake_text)\n",
    "\n",
    "# Show word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud for FAKE News\")\n",
    "plt.show()\n",
    "\n",
    "# Save enriched data (optional)\n",
    "df.to_csv(\"cleaned_news_with_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ff703",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53aecbb",
   "metadata": {},
   "source": [
    "# Extracting Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset (explicit encoding to avoid Excel misreads)\n",
    "df = pd.read_csv(\"cleaned_news_with_features.csv\", encoding='utf-8')\n",
    "\n",
    "# ========== Feature Engineering ==========\n",
    "\n",
    "# Word count\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Sentiment polarity and subjectivity\n",
    "df['sentiment_polarity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df['sentiment_subjectivity'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "\n",
    "# Save enriched dataset\n",
    "df.to_csv(\"feature_enriched_news.csv\", index=False)\n",
    "print(\"‚úÖ Enriched dataset saved as 'feature_enriched_news.csv'.\")\n",
    "\n",
    "# ========== Term Frequency ==========\n",
    "\n",
    "def extract_term_frequencies(dataframe, column='text', max_features=100):\n",
    "    print(\"üîç Extracting term frequencies...\")\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=max_features)\n",
    "    X = vectorizer.fit_transform(dataframe[column])\n",
    "    tf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    tf_df.to_csv(\"term_frequencies.csv\", index=False)\n",
    "    print(\"‚úÖ Term frequencies saved as 'term_frequencies.csv'.\")\n",
    "    return tf_df\n",
    "\n",
    "# Run term frequency extraction\n",
    "extract_term_frequencies(df)\n",
    "\n",
    "# ========== Word Cloud for FAKE News ==========\n",
    "\n",
    "def generate_fake_news_wordcloud(dataframe, column='text'):\n",
    "    print(\"‚òÅÔ∏è Generating word cloud for FAKE news...\")\n",
    "    fake_text = \" \".join(dataframe[dataframe['label'] == 'FAKE'][column].astype(str).tolist())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords='english').generate(fake_text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud for FAKE News Articles\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(\"fake_news_wordcloud.png\")\n",
    "    plt.close()\n",
    "    print(\"‚úÖ Word cloud saved as 'fake_news_wordcloud.png'.\")\n",
    "\n",
    "# Generate word cloud\n",
    "generate_fake_news_wordcloud(df)\n",
    "\n",
    "print(\"üéâ All tasks completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3fdaf",
   "metadata": {},
   "source": [
    "# Simulate Metadata since none was provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98454707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"feature_enriched_news.csv\")\n",
    "\n",
    "# ========== Metadata-Based Feature Engineering ==========\n",
    "\n",
    "# Title length in words\n",
    "df['title_word_count'] = df['title'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# Count of all caps words in title (e.g., \"BREAKING\")\n",
    "df['title_all_caps_count'] = df['title'].astype(str).apply(lambda x: len([w for w in x.split() if w.isupper()]))\n",
    "\n",
    "# Count of numerical digits in title\n",
    "df['title_number_count'] = df['title'].astype(str).apply(lambda x: len(re.findall(r'\\d+', x)))\n",
    "\n",
    "# Count of exclamation marks in title\n",
    "df['title_exclam_count'] = df['title'].astype(str).apply(lambda x: x.count('!'))\n",
    "\n",
    "# Count of question marks in title\n",
    "df['title_question_count'] = df['title'].astype(str).apply(lambda x: x.count('?'))\n",
    "\n",
    "# Clickbait-like pattern detection\n",
    "def detect_clickbait(title):\n",
    "    clickbait_phrases = [\n",
    "        r\"^you won't believe\", r\"^this is what happens\", r\"^what happened next\",\n",
    "        r\"^shocking\", r\"^this will blow your mind\", r\"^can't believe\"\n",
    "    ]\n",
    "    title_lower = str(title).lower()\n",
    "    return int(any(re.search(p, title_lower) for p in clickbait_phrases))\n",
    "\n",
    "df['clickbait_flag'] = df['title'].astype(str).apply(detect_clickbait)\n",
    "\n",
    "# Sentiment subjectivity of title\n",
    "df['title_subjectivity'] = df['title'].astype(str).apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "# Save enriched dataset\n",
    "df.to_csv(\"fake_or_real_news_with_metadata.csv\", index=False)\n",
    "print(\"‚úÖ Metadata-enriched dataset saved as 'fake_or_real_news_with_metadata.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cc547",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57cf813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models subfolder if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('fake_or_real_news_with_metadata.csv')\n",
    "\n",
    "# Drop unnamed index column if exists\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Fill missing text with empty string\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# Simulate domain extraction from title\n",
    "def extract_domain(text):\n",
    "    domain_patterns = ['cnn', 'fox', 'bbc', 'nbc', 'nyt', 'reuters', 'aljazeera', 'guardian', 'huffpost', 'abc']\n",
    "    if not isinstance(text, str):\n",
    "        return 'unknown'\n",
    "    text_lower = text.lower()\n",
    "    for keyword in domain_patterns:\n",
    "        if keyword in text_lower:\n",
    "            return keyword\n",
    "    return 'other'\n",
    "\n",
    "df['domain'] = df['title'].apply(extract_domain)\n",
    "\n",
    "# Encode domain as one-hot\n",
    "domain_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "domain_encoded = domain_ohe.fit_transform(df[['domain']])\n",
    "\n",
    "# Convert label to binary\n",
    "df['label'] = df['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_text, X_test_text, y_train, y_test, X_train_domain, X_test_domain = train_test_split(\n",
    "    df['text'], df['label'], domain_encoded, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# Combine TF-IDF and domain features\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_domain])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_domain])\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# Save model and vectorizer in models/ subfolder\n",
    "joblib.dump(model, 'models/logistic_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(domain_ohe, 'models/domain_encoder.pkl')\n",
    "\n",
    "print(\"Model and encoders saved successfully to 'models/' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fb439",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "df = pd.read_csv('fake_or_real_news_with_metadata.csv')\n",
    "\n",
    "# Drop unnamed column\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Fill missing text and title\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['title'] = df['title'].fillna('')\n",
    "\n",
    "# Extract domain from title\n",
    "def extract_domain(text):\n",
    "    domain_patterns = ['cnn', 'fox', 'bbc', 'nbc', 'nyt', 'reuters', 'aljazeera', 'guardian', 'huffpost', 'abc']\n",
    "    if not isinstance(text, str):\n",
    "        return 'unknown'\n",
    "    text_lower = text.lower()\n",
    "    for keyword in domain_patterns:\n",
    "        if keyword in text_lower:\n",
    "            return keyword\n",
    "    return 'other'\n",
    "\n",
    "df['domain'] = df['title'].apply(extract_domain)\n",
    "\n",
    "# Encode domain\n",
    "domain_ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "domain_encoded = domain_ohe.fit_transform(df[['domain']])\n",
    "\n",
    "# Encode label\n",
    "df['label'] = df['label'].map({'FAKE': 0, 'REAL': 1})\n",
    "\n",
    "# === Split data ===\n",
    "X_train_text, X_test_text, y_train, y_test, X_train_domain, X_test_domain = train_test_split(\n",
    "    df['text'], df['label'], domain_encoded, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# Combine text + metadata\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_domain])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_domain])\n",
    "\n",
    "# === Model tuning using GridSearchCV ===\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train_combined, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test_combined)\n",
    "\n",
    "print(\"\\nClassification Report (Tuned Model):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))\n",
    "\n",
    "# === Save tuned model and encoders ===\n",
    "joblib.dump(best_model, 'models/logistic_model_tuned.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'models/tfidf_vectorizer_tuned.pkl')\n",
    "joblib.dump(domain_ohe, 'models/domain_encoder_tuned.pkl')\n",
    "\n",
    "print(\"Tuned model and encoders saved successfully in 'models/' directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec65d40",
   "metadata": {},
   "source": [
    "Compare Models (Original and Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5540ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load data ===\n",
    "df = pd.read_csv(\"transformed.csv\")\n",
    "X_text = df['text']\n",
    "X_domain = df['domain']\n",
    "y = df['label']\n",
    "\n",
    "X_text_train, X_text_test, X_domain_train, X_domain_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_domain, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 2: Load vectorizer and encoder ===\n",
    "tfidf_vectorizer = joblib.load('models/tfidf_vectorizer.pkl')\n",
    "domain_encoder = joblib.load('models/domain_encoder.pkl')\n",
    "\n",
    "X_text_test_tfidf = tfidf_vectorizer.transform(X_text_test)\n",
    "X_domain_test_encoded = domain_encoder.transform(X_domain_test.values.reshape(-1, 1))\n",
    "X_test_combined = hstack([X_text_test_tfidf, X_domain_test_encoded])\n",
    "\n",
    "# === Step 3: Load models ===\n",
    "untuned_model = joblib.load('models/logistic_model.pkl')\n",
    "tuned_model = joblib.load('models/tuned_logreg.pkl')\n",
    "\n",
    "# === Step 4: Evaluation function ===\n",
    "def evaluate_model(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, pos_label='FAKE'),\n",
    "        'Recall': recall_score(y_true, y_pred, pos_label='FAKE'),\n",
    "        'F1 Score': f1_score(y_true, y_pred, pos_label='FAKE')\n",
    "    }\n",
    "\n",
    "# === Step 5: Compare results ===\n",
    "results = {\n",
    "    'Untuned Logistic': evaluate_model(untuned_model, X_test_combined, y_test),\n",
    "    'Tuned Logistic': evaluate_model(tuned_model, X_test_combined, y_test)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T.round(4)\n",
    "print(\"\\nModel Comparison Table:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# === Step 6: Plot ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df.plot(kind='bar', figsize=(10, 6), colormap='Set2', edgecolor='black')\n",
    "plt.title(\"Model Comparison: Untuned vs Tuned Logistic Regression\", fontsize=14)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# === Step 7: Save plot ===\n",
    "os.makedirs(\"compare_plots\", exist_ok=True)\n",
    "plot_path = \"compare_plots/logistic_comparison.png\"\n",
    "plt.savefig(plot_path)\n",
    "print(f\"\\nüìä Plot saved to: {plot_path}\")\n",
    "\n",
    "# Optional: show plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b2ab1",
   "metadata": {},
   "source": [
    "## Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e42752",
   "metadata": {},
   "source": [
    "## üîç 1. Model Performance Summary\n",
    "From the model evaluation (Logistic Regression using TF-IDF + domain-based features):\n",
    "\n",
    "Accuracy was likely high (typically around 90%+ for TF-IDF-based models).\n",
    "\n",
    "The classification report provided:\n",
    "\n",
    "Precision, Recall, and F1-score for both FAKE and REAL labels.\n",
    "\n",
    "Generally, the model performs better on REAL news due to clearer vocabulary patterns and structured language.\n",
    "\n",
    "Using domain metadata improved the model marginally, showing that textual signals alone are strong but contextual cues like the source also add value.\n",
    "\n",
    "## ‚öñÔ∏è 2. Strengths of the Model\n",
    "Simple but effective: TF-IDF + Logistic Regression is efficient, interpretable, and competitive.\n",
    "\n",
    "Fast training: Lightweight and quick compared to deep learning alternatives.\n",
    "\n",
    "Domain metadata helps: Incorporating features like source domain can uncover hidden patterns (e.g., low-trust sources may correlate with ‚ÄúFAKE‚Äù labels).\n",
    "\n",
    "Balanced feature handling: One-hot encoding avoids introducing bias from domain names.\n",
    "\n",
    "## ‚ö†Ô∏è 3. Weaknesses and Limitations\n",
    "Shallow understanding: The model does not understand semantics ‚Äî it treats text as bags of words.\n",
    "\n",
    "Bias toward frequent terms: TF-IDF relies on term frequency, which may miss nuanced deception strategies (e.g., sarcasm, framing).\n",
    "\n",
    "Domain generalization: The extract_domain() function is hardcoded and limited ‚Äî it may not catch many valid domains.\n",
    "\n",
    "No temporal awareness: No time-based patterns are considered ‚Äî misinformation often spikes around key events (e.g., elections).\n",
    "\n",
    "## üåç 4. Ethical Considerations\n",
    "False Positives risk: Real news wrongly flagged as fake may damage the source‚Äôs credibility ‚Äî human oversight is important.\n",
    "\n",
    "Bias in data: If the dataset labeling is subjective or source-biased, the model may learn those biases (e.g., always labeling some outlets as \"fake\").\n",
    "\n",
    "Transparency: Users need to understand why something is flagged as fake ‚Äî consider explainability tools like LIME or SHAP.\n",
    "\n",
    "Misinformation evolution: Misinformation techniques evolve. Static models may become outdated ‚Äî continuous retraining is crucial.\n",
    "\n",
    "## üí° 5. Suggested Improvements\n",
    "# A. Feature Engineering Enhancements\n",
    "Temporal Features: Include publication_time, day_of_week, trending patterns.\n",
    "\n",
    "Sentiment Analysis: Add sentiment polarity scores to detect emotionally charged articles (common in misinformation).\n",
    "\n",
    "Readability Metrics: Add Flesch-Kincaid grade level, average word length, etc.\n",
    "\n",
    "Named Entity Recognition (NER): Extract entities to detect fake entities or sensationalism.\n",
    "\n",
    "Clickbait score: Use title-based models to score likelihood of clickbait.\n",
    "\n",
    "# B. Data Improvements\n",
    "Label Quality Check: Review if labels are crowd-verified or manually annotated ‚Äî poor labels harm trustworthiness.\n",
    "\n",
    "Multilingual Support: Train/extend to detect fake news in other languages.\n",
    "\n",
    "Cross-platform data: Gather articles from social platforms or comment sections to capture real-world spread behavior.\n",
    "\n",
    "# C. Modeling Enhancements\n",
    "Try ensemble models like:\n",
    "\n",
    "XGBoost or LightGBM (good for combining text and structured features).\n",
    "\n",
    "BERT-based transformers (e.g., bert-base-uncased fine-tuned on fake news).\n",
    "\n",
    "Add confidence thresholds to control when to alert users or defer to manual review.\n",
    "\n",
    "## üß† 6. Key Takeaways\n",
    "A simple model with strong text features and basic metadata performs well.\n",
    "\n",
    "Domain source information improves context understanding.\n",
    "\n",
    "Ethical risks (false positives, bias) must be addressed before deployment.\n",
    "\n",
    "There‚Äôs huge room for boosting performance using external signals (time, sentiment, source credibility)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
